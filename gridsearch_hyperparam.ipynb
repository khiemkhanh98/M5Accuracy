{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "import pandas as pd\n",
    "import dataWrangling\n",
    "from memoryReduction import *\n",
    "import gc\n",
    "import importlib\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score\n",
    "import multiprocessing\n",
    "sns.set()\n",
    "print(multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = dataWrangling.create_dt(is_train=True, first_day= 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "%timeit \n",
    "\n",
    "import multiprocessing as mp\n",
    "importlib.reload(dataWrangling)\n",
    "#p = mp.Pool(2)\n",
    "df = dataWrangling.create_fea(df)\n",
    "df = reduce_mem_usage(df)\n",
    "df.to_pickle(\"data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"./dummy.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_feats = ['item_id', 'dept_id','store_id', 'cat_id', 'state_id'] + \\\n",
    "            [\"event_name_1\", \"event_name_2\", \"event_type_1\", \"event_type_2\"] +\\\n",
    "            ['year','month','week','day','dayofweek']\n",
    "useless_cols = [\"id\", \"date\", \"sales\",\"d\", \"wm_yr_wk\"]\n",
    "train_cols = df.columns[~df.columns.isin(useless_cols)]\n",
    "X_train = df[train_cols]\n",
    "y_train = df[\"sales\"]\n",
    "X_train = reduce_mem_usage(X_train)\n",
    "y_train = reduce_mem_usage(y_train)\n",
    "\n",
    "del df\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "train_data = lgb.Dataset(X_train, label = y_train, categorical_feature=cat_feats, free_raw_data=False)\n",
    "fake_valid_inds = np.random.choice(len(X_train), 500000)\n",
    "fake_valid_data = lgb.Dataset(X_train.iloc[fake_valid_inds], label = y_train.iloc[fake_valid_inds],categorical_feature=cat_feats,\n",
    "                             free_raw_data=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#taken from https://www.kaggle.com/bitit1994/parameter-grid-search-lgbm-with-scikit-learn\n",
    "'''\n",
    "params = {\n",
    "        \"objective\" : \"poisson\",\n",
    "        \"metric\" :\"rmse\",\n",
    "        \"force_row_wise\" : True,\n",
    "        \"learning_rate\" : 0.075,\n",
    "#         \"sub_feature\" : 0.8,\n",
    "        \"sub_row\" : 0.75,\n",
    "        \"bagging_freq\" : 1,\n",
    "        \"lambda_l2\" : 0.1,\n",
    "#         \"nthread\" : 4\n",
    "        'verbosity': 1,\n",
    "        'num_iterations' : 25,\n",
    "}\n",
    "'''\n",
    "# Initiate classifier to use\n",
    "mdl = lgb.LGBMClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridParams = {\n",
    "    'learning_rate': [0.005, 0.01,0.05],\n",
    "#    'n_estimators': [8,16,24],\n",
    "    'num_leaves': [12,48,86], # large num_leaves helps improve accuracy but might lead to over-fitting\n",
    "    'boosting_type' : ['gbdt', 'dart'], # for better accuracy -> try dart\n",
    "    'objective' : ['poisson','regression'],\n",
    "    'max_bin':[255, 510], # large max_bin helps improve accuracy but might slow down training progress\n",
    "    'random_state' : [500],\n",
    "    \"sub_feature\" : [0.9,0.8,0.7],\n",
    "    \"sub_row\" : [0.75,0.8,0.85],\n",
    "    'colsample_bytree' : [0.64, 0.65, 0.66],\n",
    "    'subsample' : [0.7,0.75,0.8],\n",
    "    'lambda_l1' : [0.1,0.05,0.15,0.5,1,1.2],\n",
    "    \"lambda_l2\" : [0.1,0.05,0.15,0.5,1,1.2],\n",
    "    \"metric\" : ['rmse'],\n",
    "    'num_iterations' : [100],\n",
    "    'verbosity': [1],\n",
    "    \"force_row_wise\" : [True],\n",
    "    \"bagging_freq\" : [1],\n",
    "    }\n",
    "\n",
    "grid = GridSearchCV(mdl, gridParams, verbose=1, n_jobs=4,cv = 4) ##remember to add cv\n",
    "# Run the grid\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters found\n",
    "print(grid.best_params_)\n",
    "print(grid.best_score_)\n",
    "\n",
    "file = open(\"params.txt\",\"w\") \n",
    "file.writelines(grid.best_params_) \n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = lgb.Dataset(X_train, label = y_train, categorical_feature=cat_feats, free_raw_data=False)\n",
    "fake_valid_inds = np.random.choice(len(X_train), 500000)\n",
    "fake_valid_data = lgb.Dataset(X_train.iloc[fake_valid_inds], label = y_train.iloc[fake_valid_inds],categorical_feature=cat_feats,\n",
    "                             free_raw_data=False)\n",
    "del X_train, y_train\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'learning_rate': grid.best_params_['colsample_bytree'],\n",
    "    'num_leaves': grid.best_params_['num_leaves'], # large num_leaves helps improve accuracy but might lead to over-fitting\n",
    "    'boosting_type' : grid.best_params_['boosting_type'], # for better accuracy -> try dart\n",
    "    'objective' : grid.best_params_['objective'],\n",
    "    'max_bin': grid.best_params_['max_bin'], # large max_bin helps improve accuracy but might slow down training progress\n",
    "    'random_state' : grid.best_params_['random_state'],\n",
    "    \"sub_feature\" : grid.best_params_['sub_feature'],\n",
    "    \"sub_row\" : grid.best_params_['sub_row'],\n",
    "    'colsample_bytree' : grid.best_params_['colsample_bytree'],\n",
    "    'subsample' : grid.best_params_['subsample'],\n",
    "    'lambda_l1' : grid.best_params_['lambda_l1'],\n",
    "    \"lambda_l2\" : grid.best_params_['lambda_l2'],\n",
    "    \"metric\" : grid.best_params_['metric'],\n",
    "    'num_iterations' : [\"2500\"],\n",
    "    'verbosity': grid.best_params_['verbosity'],\n",
    "    \"force_row_wise\" : grid.best_params_['force_row_wise'],\n",
    "    \"bagging_freq\" : grid.best_params_['bagging_freq'],\n",
    "}\n",
    "\n",
    "#Initiate classifier to use\n",
    "m_lgb = lgb.train(params, train_data, valid_sets = [fake_valid_data], verbose_eval=100) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_lgb.save_model(\"model.lgb\")\n",
    "\n",
    "del train_data,fake_valid_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 28 \n",
    "max_lags = 366\n",
    "tr_last = 1913\n",
    "fday = datetime(2016,4, 25) \n",
    "te = create_dt(False)\n",
    "cols = [f\"F{i}\" for i in range(1,29)]\n",
    "\n",
    "for tdelta in range(0, 28):\n",
    "    day = fday + timedelta(days=tdelta)\n",
    "    print(day)\n",
    "    tst = te[(te.date >= day - timedelta(days=max_lags)) & (te.date <= day)].copy()\n",
    "    create_fea(tst)\n",
    "    tst = tst.loc[tst.date == day , train_cols]\n",
    "    te.loc[te.date == day, \"sales\"] = m_lgb.predict(tst) # magic multiplier by kyakovlev\n",
    "\n",
    "\n",
    "\n",
    "te_sub = te.loc[te.date >= fday, [\"id\", \"sales\"]].copy()\n",
    "te_sub[\"F\"] = [f\"F{rank}\" for rank in te_sub.groupby(\"id\")[\"id\"].cumcount()+1]\n",
    "te_sub = te_sub.set_index([\"id\", \"F\" ]).unstack()[\"sales\"][cols].reset_index()\n",
    "te_sub.fillna(0., inplace = True)\n",
    "te_sub.sort_values(\"id\", inplace = True)\n",
    "te_sub.reset_index(drop=True, inplace = True)\n",
    "te_sub.to_csv(f\"submission_{icount}.csv\",index=False)\n",
    "sub2 = te_sub.copy()\n",
    "sub2[\"id\"] = sub2[\"id\"].str.replace(\"validation$\", \"evaluation\")\n",
    "sub = pd.concat([sub, sub2], axis=0, sort=False)\n",
    "sub.to_csv(\"submission.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
